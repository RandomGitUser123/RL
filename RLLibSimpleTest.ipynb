{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d79d017a-9c10-45fe-8379-eb07b3351952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1633457d-5086-448a-a463-36f2927fefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloatMatchEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"console\"]}\n",
    "\n",
    "    def __init__(self, render_mode=\"console\"):\n",
    "        super(FloatMatchEnv, self).__init__()\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "        self.nextFloat = np.random.ranf(1,).astype(np.float32)\n",
    "        self.memory = pd.DataFrame(columns=[\"Step\", \"Previous Observation\", \"Action\", \"Reward\"])\n",
    "        self.memory.index.name = \"Step\"\n",
    "        self.currentStep = 0\n",
    "        self.maxSteps = 1000\n",
    "        self.tolerance = 0.05\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.currentStep = 0\n",
    "        self.action = 0\n",
    "        self.nextFloat = np.array(np.random.ranf(1,)).astype(np.float32)\n",
    "        self.memory = pd.DataFrame(columns=[\"Previous Observation\", \"Action\", \"Reward\"])\n",
    "        self.memory.index.name = \"Step\"\n",
    "\n",
    "        observation = self.nextFloat\n",
    "\n",
    "        return observation, {}  # empty info dict\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.action = action.item()\n",
    "            # Calculate the absolute error between predicted and target values\n",
    "        error = abs(self.action - self.nextFloat)\n",
    "    \n",
    "        # Check if the predicted value is within the tolerance range\n",
    "        if error <= self.tolerance:\n",
    "            reward = 1.0  # Maximum reward for accurate predictions\n",
    "        else:\n",
    "            reward = max(0.0, (1.0 - error)**2)\n",
    "            reward = reward.item()\n",
    "        self.memory = pd.DataFrame(columns=[\"Previous Observation\", \"Action\", \"Reward\"])\n",
    "        self.memory.loc[self.currentStep] = [self.nextFloat, self.action, reward]\n",
    "        self.nextFloat = np.random.ranf(1,).astype(np.float32)\n",
    "        observation = self.nextFloat    \n",
    "        self.currentStep = self.currentStep + 1       \n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        if self.currentStep > self.maxSteps:\n",
    "            terminated = True\n",
    "            \n",
    "\n",
    "        return (\n",
    "            observation,\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            self.memory.to_dict(),\n",
    "        )\n",
    "         \n",
    "    def render(self):\n",
    "        # agent is represented as a cross, rest as a dot\n",
    "        if self.render_mode == \"console\":\n",
    "           #print(self.date)\n",
    "            print(self.memory.iloc[-1:].to_string(index=True, header=True))\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8016e0b9-d63d-4cdc-bba5-e15ed0aa596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "register_env(\"FloatMatchEnv\", FloatMatchEnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59919f33-c2a4-4ff7-8547-58d2bb5352ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 15:26:48,870\tWARNING algorithm_config.py:797 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "C:\\Users\\flori\\anaconda3\\envs\\py310\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "C:\\Users\\flori\\anaconda3\\envs\\py310\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "C:\\Users\\flori\\anaconda3\\envs\\py310\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "C:\\Users\\flori\\anaconda3\\envs\\py310\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "\u001b[36m(RolloutWorker pid=35036)\u001b[0m 2023-11-12 15:27:00,061\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n",
      "\u001b[36m(RolloutWorker pid=31864)\u001b[0m 2023-11-12 15:27:00,095\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "2023-11-12 15:27:00,166\tWARNING algorithm_config.py:797 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "2023-11-12 15:27:07,323\tINFO trainable.py:164 -- Trainable.setup took 18.433 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-11-12 15:27:07,326\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n",
      "\u001b[36m(RolloutWorker pid=10556)\u001b[0m 2023-11-12 15:27:07,237\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=10556)\u001b[0m 2023-11-12 15:27:07,263\tWARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'sampler_results': {'episode_reward_max': 532.2578395393148,\n",
       "   'episode_reward_min': 508.26897799695143,\n",
       "   'episode_reward_mean': 519.0193392155858,\n",
       "   'episode_len_mean': 1001.0,\n",
       "   'episode_media': {},\n",
       "   'episodes_this_iter': 10,\n",
       "   'policy_reward_min': {},\n",
       "   'policy_reward_max': {},\n",
       "   'policy_reward_mean': {},\n",
       "   'custom_metrics': {},\n",
       "   'hist_stats': {'episode_reward': [532.2578395393148,\n",
       "     514.1613608431358,\n",
       "     522.1899891095527,\n",
       "     513.2495196799509,\n",
       "     524.7666362208038,\n",
       "     519.5802247530846,\n",
       "     508.26897799695143,\n",
       "     514.006536446941,\n",
       "     518.8330887061456,\n",
       "     522.8792188599764],\n",
       "    'episode_lengths': [1001,\n",
       "     1001,\n",
       "     1001,\n",
       "     1001,\n",
       "     1001,\n",
       "     1001,\n",
       "     1001,\n",
       "     1001,\n",
       "     1001,\n",
       "     1001]},\n",
       "   'sampler_perf': {'mean_raw_obs_processing_ms': 0.55759815935869,\n",
       "    'mean_inference_ms': 1.5768755709966595,\n",
       "    'mean_action_processing_ms': 0.20032281011578182,\n",
       "    'mean_env_wait_ms': 1.4055596163860868,\n",
       "    'mean_env_render_ms': 0.0},\n",
       "   'num_faulty_episodes': 0,\n",
       "   'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0,\n",
       "    'StateBufferConnector_ms': 0.0,\n",
       "    'ViewRequirementAgentConnector_ms': 0.3593611717224121}},\n",
       "  'episode_reward_max': 532.2578395393148,\n",
       "  'episode_reward_min': 508.26897799695143,\n",
       "  'episode_reward_mean': 519.0193392155858,\n",
       "  'episode_len_mean': 1001.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 10,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [532.2578395393148,\n",
       "    514.1613608431358,\n",
       "    522.1899891095527,\n",
       "    513.2495196799509,\n",
       "    524.7666362208038,\n",
       "    519.5802247530846,\n",
       "    508.26897799695143,\n",
       "    514.006536446941,\n",
       "    518.8330887061456,\n",
       "    522.8792188599764],\n",
       "   'episode_lengths': [1001,\n",
       "    1001,\n",
       "    1001,\n",
       "    1001,\n",
       "    1001,\n",
       "    1001,\n",
       "    1001,\n",
       "    1001,\n",
       "    1001,\n",
       "    1001]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.55759815935869,\n",
       "   'mean_inference_ms': 1.5768755709966595,\n",
       "   'mean_action_processing_ms': 0.20032281011578182,\n",
       "   'mean_env_wait_ms': 1.4055596163860868,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0,\n",
       "   'StateBufferConnector_ms': 0.0,\n",
       "   'ViewRequirementAgentConnector_ms': 0.3593611717224121},\n",
       "  'num_agent_steps_sampled_this_iter': 10010,\n",
       "  'num_env_steps_sampled_this_iter': 10010,\n",
       "  'timesteps_this_iter': 10010}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "\n",
    "config = (  # 1. Configure the algorithm,\n",
    "    PPOConfig()\n",
    "    .environment(FloatMatchEnv)\n",
    "    .rollouts(num_rollout_workers=2)\n",
    "    .framework(\"torch\")\n",
    "    .training()\n",
    "    .evaluation(evaluation_num_workers=1)\n",
    ")\n",
    "#pretty_print(config.to_dict())\n",
    "\n",
    "algo = config.build()  # 2. build the algorithm,\n",
    "\n",
    "for _ in range(4):\n",
    "    result = algo.train()  # 3. train it,\n",
    "\n",
    "#pretty_print(result)\n",
    "algo.evaluate()  # 4. and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d3d41f7-a0d2-464c-a5d0-d42e0366e7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Previous Observation, Action, Reward]\n",
      "Index: []\n",
      "  Previous Observation    Action  Reward\n",
      "0         [0.80681854]  0.815053     1.0\n",
      "  Previous Observation   Action   Reward\n",
      "1        [0.013449535]  0.71724  0.08774\n",
      "  Previous Observation    Action    Reward\n",
      "2           [0.779399]  0.674487  0.801182\n",
      "  Previous Observation    Action  Reward\n",
      "3         [0.11691102]  0.128062     1.0\n",
      "  Previous Observation   Action    Reward\n",
      "4          [0.9139152]  0.98967  0.854229\n",
      "  Previous Observation  Action    Reward\n",
      "5         [0.28003678]     1.0  0.078421\n",
      "  Previous Observation  Action    Reward\n",
      "6         [0.91142297]     1.0  0.830692\n",
      "  Previous Observation    Action    Reward\n",
      "7         [0.35351092]  0.299932  0.895713\n",
      "  Previous Observation    Action   Reward\n",
      "8          [0.9563749]  0.448056  0.24175\n",
      "  Previous Observation  Action    Reward\n",
      "9          [0.9498008]     1.0  0.902122\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "n_steps = 10\n",
    "for step in range(n_steps):\n",
    "    action = algo.compute_single_action(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    log = info\n",
    "    done = terminated or truncated\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d63e238-6741-4874-bf9c-ae1a373e82c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
